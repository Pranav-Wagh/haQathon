{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e1b0206-2912-4385-97b9-5948ed70dfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HAQKATHON SCL\\haQathon\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp #face detector\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import tkinter for GUI\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, scrolledtext, filedialog\n",
    "import PIL.Image, PIL.ImageTk\n",
    "from threading import Thread, Event\n",
    "import os\n",
    "\n",
    "# Additional imports for audio processing\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import queue\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# Import transformers for audio processing\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set up ONNX Runtime for Hexagon\n",
    "ONNXRUNTIME_DIR = Path(ort.__file__).parent\n",
    "\n",
    "HEXAGON_DRIVER = Path.joinpath(ONNXRUNTIME_DIR, \"capi\", \"QnnHtp.dll\")\n",
    "\n",
    "QNN_PROVIDER_OPTIONS = {\"backend_path\": HEXAGON_DRIVER,}\n",
    "\n",
    "SESS_OPTS = ort.SessionOptions()\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0907155",
   "metadata": {},
   "source": [
    "#### Model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcf9fa-a7cc-4d4c-b723-6d7efd49b94b",
   "metadata": {},
   "source": [
    "#### Sub functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0fc324-98a8-4efc-bb11-4bec8a015790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pth_processing_np(fp):\n",
    "    \"\"\"\n",
    "    Numpy version of pth_processing function\n",
    "    Performs the same preprocessing as the PyTorch version but using numpy\n",
    "    \n",
    "    Args:\n",
    "        fp: PIL Image or numpy array\n",
    "    \n",
    "    Returns:\n",
    "        numpy array with shape (1, 3, 224, 224) ready for model input\n",
    "    \"\"\"\n",
    "    def preprocess_input_np(img_array):\n",
    "        \"\"\"Apply the same preprocessing as PyTorch version\"\"\"\n",
    "        # Convert to float32\n",
    "        x = img_array.astype(np.float32)\n",
    "        \n",
    "        # Flip channels (RGB -> BGR, equivalent to torch.flip(dims=(0,)))\n",
    "        x = x[:, :, ::-1]\n",
    "        \n",
    "        # Apply normalization (same values as PyTorch version)\n",
    "        x[:, :, 0] -= 91.4953   # Blue channel\n",
    "        x[:, :, 1] -= 103.8827  # Green channel  \n",
    "        x[:, :, 2] -= 131.0912  # Red channel\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_img_numpy(img):\n",
    "        \"\"\"Convert PIL image to preprocessed numpy array\"\"\"\n",
    "        # Resize image to 224x224 (same as PyTorch version)\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        \n",
    "        # Convert PIL to numpy array\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        img_array = preprocess_input_np(img_array)\n",
    "        \n",
    "        # Transpose from HWC to CHW format (Height, Width, Channels -> Channels, Height, Width)\n",
    "        img_array = np.transpose(img_array, (2, 0, 1))\n",
    "        \n",
    "        # Add batch dimension (equivalent to torch.unsqueeze(img, 0))\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        return img_array\n",
    "    \n",
    "    return get_img_numpy(fp)\n",
    "\n",
    "def norm_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    \n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    \n",
    "    return x_px, y_px\n",
    "\n",
    "def get_box(fl, w, h):\n",
    "    idx_to_coors = {}\n",
    "    for idx, landmark in enumerate(fl.landmark):\n",
    "        landmark_px = norm_coordinates(landmark.x, landmark.y, w, h)\n",
    "\n",
    "        if landmark_px:\n",
    "            idx_to_coors[idx] = landmark_px\n",
    "\n",
    "    x_min = np.min(np.asarray(list(idx_to_coors.values()))[:,0])\n",
    "    y_min = np.min(np.asarray(list(idx_to_coors.values()))[:,1])\n",
    "    endX = np.max(np.asarray(list(idx_to_coors.values()))[:,0])\n",
    "    endY = np.max(np.asarray(list(idx_to_coors.values()))[:,1])\n",
    "\n",
    "    (startX, startY) = (max(0, x_min), max(0, y_min))\n",
    "    (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "    \n",
    "    return startX, startY, endX, endY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04b4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVisualizer:\n",
    "    def __init__(self, frame, height=100, width=200):\n",
    "        self.frame = frame\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Create matplotlib figure\n",
    "        self.fig = Figure(figsize=(width/100, height/100), dpi=100)\n",
    "        self.ax = self.fig.add_subplot(111)\n",
    "        \n",
    "        # Initial empty plot\n",
    "        self.line, = self.ax.plot([], [], color='#3498db', linewidth=2)\n",
    "        self.ax.set_ylim(-0.5, 0.5)\n",
    "        self.ax.set_xlim(0, 100)\n",
    "        self.ax.axis('off')\n",
    "        self.fig.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0, hspace=0)\n",
    "        \n",
    "        # Create canvas\n",
    "        self.canvas = FigureCanvasTkAgg(self.fig, master=self.frame)\n",
    "        self.canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Buffer for audio data\n",
    "        self.buffer = [0] * 100\n",
    "    \n",
    "    def update(self, audio_data):\n",
    "        # Convert audio data to normalized values\n",
    "        if audio_data is not None and len(audio_data) > 0:\n",
    "            # Ensure we're getting the right number of samples\n",
    "            samples = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32)\n",
    "            samples = samples / 32768.0  # Normalize\n",
    "            \n",
    "            # Update buffer (sliding window)\n",
    "            self.buffer = self.buffer[len(samples):] + samples.tolist()[:100]\n",
    "            \n",
    "            # Update plot\n",
    "            self.line.set_data(range(len(self.buffer)), self.buffer)\n",
    "            self.canvas.draw_idle()\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, visualizer=None, callback=None):\n",
    "        self.CHUNK = 1024\n",
    "        self.FORMAT = pyaudio.paInt16\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = 16000  # 16kHz for model\n",
    "        self.recording = False\n",
    "        self.frames = []\n",
    "        self.visualizer = visualizer\n",
    "        self.callback = callback\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        self.stream = None\n",
    "        \n",
    "        # Speech characteristics\n",
    "        self.speech_metrics = {\n",
    "            \"avg_pitch\": 0,\n",
    "            \"pitch_variation\": 0,\n",
    "            \"speaking_rate\": 0,\n",
    "            \"volume\": 0,\n",
    "            \"clarity\": 0,\n",
    "            \"samples_processed\": 0\n",
    "        }\n",
    "        \n",
    "        # Voice emotions (derived from metrics only)\n",
    "        self.voice_emotions = {\n",
    "            \"confidence\": 0,\n",
    "            \"engagement\": 0,\n",
    "            \"hesitation\": 0,\n",
    "            \"enthusiasm\": 0\n",
    "        }\n",
    "        \n",
    "        # Flag for periodic processing\n",
    "        self.last_process_time = 0\n",
    "        self.processing_interval = 5  # Process every 5 seconds\n",
    "\n",
    "    def start_recording(self):\n",
    "        if self.recording:\n",
    "            return\n",
    "        self.recording = True\n",
    "        self.frames = []\n",
    "        self.last_process_time = time.time()\n",
    "        \n",
    "        # Reset metrics\n",
    "        self.speech_metrics = {\n",
    "            \"avg_pitch\": 0,\n",
    "            \"pitch_variation\": 0,\n",
    "            \"speaking_rate\": 0,\n",
    "            \"volume\": 0,\n",
    "            \"clarity\": 0,\n",
    "            \"samples_processed\": 0\n",
    "        }\n",
    "        \n",
    "        self.voice_emotions = {\n",
    "            \"confidence\": 0,\n",
    "            \"engagement\": 0,\n",
    "            \"hesitation\": 0,\n",
    "            \"enthusiasm\": 0\n",
    "        }\n",
    "        \n",
    "        def callback(in_data, frame_count, time_info, status):\n",
    "            if self.recording:\n",
    "                self.frames.append(in_data)\n",
    "                self.audio_queue.put(in_data)\n",
    "                \n",
    "                # Update visualizer if available\n",
    "                if self.visualizer:\n",
    "                    self.visualizer.update(in_data)\n",
    "                \n",
    "                # Process audio periodically\n",
    "                current_time = time.time()\n",
    "                if current_time - self.last_process_time >= self.processing_interval:\n",
    "                    self.process_audio_chunk()\n",
    "                    self.last_process_time = current_time\n",
    "            return (in_data, pyaudio.paContinue)\n",
    "        \n",
    "        try:\n",
    "            self.stream = self.audio.open(\n",
    "                format=self.FORMAT,\n",
    "                channels=self.CHANNELS,\n",
    "                rate=self.RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=self.CHUNK,\n",
    "                stream_callback=callback\n",
    "            )\n",
    "            print(\"Audio recording started\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error starting audio stream: {e}\")\n",
    "            self.recording = False\n",
    "\n",
    "    def stop_recording(self):\n",
    "        if not self.recording:\n",
    "            return\n",
    "        self.recording = False\n",
    "        \n",
    "        if self.stream:\n",
    "            try:\n",
    "                self.stream.stop_stream()\n",
    "                self.stream.close()\n",
    "                self.stream = None\n",
    "                print(\"Audio recording stopped\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error stopping audio stream: {e}\")\n",
    "\n",
    "    def process_audio_chunk(self):\n",
    "        # Always process/log, even if not enough frames\n",
    "        enough_data = len(self.frames) >= 50\n",
    "        # Take last ~3 seconds of audio, or whatever is available\n",
    "        recent_frames = self.frames[-50:] if enough_data else self.frames\n",
    "        try:\n",
    "            if recent_frames:\n",
    "                audio_data = np.frombuffer(b''.join(recent_frames), dtype=np.int16).astype(np.float32)\n",
    "                audio_data = audio_data / 32768.0  # Normalize to [-1, 1]\n",
    "            else:\n",
    "                audio_data = np.array([])\n",
    "\n",
    "            # Only use direct speech metrics (no HuggingFace model)\n",
    "            direct_metrics_emotions = {\n",
    "                \"confidence\": 0,\n",
    "                \"engagement\": 0,\n",
    "                \"hesitation\": 0,\n",
    "                \"enthusiasm\": 0\n",
    "            }\n",
    "\n",
    "            if len(audio_data) > 0:\n",
    "                # Calculate volume (RMS amplitude)\n",
    "                volume = np.sqrt(np.mean(audio_data**2))\n",
    "                # In AudioProcessor.process_audio_chunk, after volume calculation\n",
    "                if volume > 0:\n",
    "                    volume_db = 20 * np.log10(volume)\n",
    "                else:\n",
    "                    volume_db = -100.0\n",
    "                self.speech_metrics[\"volume_db\"] = volume_db\n",
    "                try:\n",
    "                    # Extract pitch information\n",
    "                    pitches, magnitudes = librosa.piptrack(y=audio_data, sr=self.RATE)\n",
    "                    pitch_values = []\n",
    "                    for i in range(pitches.shape[1]):\n",
    "                        index = magnitudes[:, i].argmax()\n",
    "                        pitch = pitches[index, i]\n",
    "                        if pitch > 0:\n",
    "                            pitch_values.append(pitch)\n",
    "                    \n",
    "                    if pitch_values:\n",
    "                        # Calculate basic speech metrics\n",
    "                        avg_pitch = np.mean(pitch_values)\n",
    "                        pitch_variation = np.std(pitch_values)\n",
    "                        alpha = 0.7  # Smoothing factor\n",
    "                        \n",
    "                        # Update speech metrics with new values\n",
    "                        self.speech_metrics[\"avg_pitch\"] = alpha * avg_pitch + (1 - alpha) * self.speech_metrics[\"avg_pitch\"]\n",
    "                        self.speech_metrics[\"pitch_variation\"] = alpha * pitch_variation + (1 - alpha) * self.speech_metrics[\"pitch_variation\"]\n",
    "                        self.speech_metrics[\"volume\"] = alpha * volume + (1 - alpha) * self.speech_metrics[\"volume\"]\n",
    "                        self.speech_metrics[\"samples_processed\"] += 1\n",
    "                        \n",
    "                        # Calculate speaking rate from zero crossings\n",
    "                        zero_crossings = librosa.zero_crossings(audio_data)\n",
    "                        speaking_rate = sum(zero_crossings) / len(audio_data) * self.RATE / 100\n",
    "                        self.speech_metrics[\"speaking_rate\"] = alpha * speaking_rate + (1 - alpha) * self.speech_metrics[\"speaking_rate\"]\n",
    "                        \n",
    "                        # Clarity (spectral centroid)\n",
    "                        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=self.RATE)[0]\n",
    "                        self.speech_metrics[\"clarity\"] = min(1.0, np.mean(spectral_centroids) / 3000)\n",
    "                        \n",
    "                        # Calculate emotions from direct speech metrics\n",
    "                        # 1. Confidence: Based on volume and steady pitch\n",
    "                        norm_pitch_var = min(1.0, pitch_variation / 200)  # Cap at 200Hz variation\n",
    "                        steady_factor = 1.0 - norm_pitch_var * 0.5  # Steadier voice = more confident (but with less impact)\n",
    "                        volume_factor = min(1.0, volume * 12)  # Louder = more confident, up to a limit\n",
    "                        direct_metrics_emotions[\"confidence\"] = 0.8 * volume_factor + 0.2 * steady_factor\n",
    "                        \n",
    "                        # 2. Enthusiasm: Based on pitch variation and speaking rate\n",
    "                        norm_speaking_rate = min(1.0, speaking_rate / 15)  # Good rate is around 10-15\n",
    "                        direct_metrics_emotions[\"enthusiasm\"] = 0.6 * norm_pitch_var + 0.4 * norm_speaking_rate\n",
    "                        \n",
    "                        # 3. Hesitation: Inverse of speaking rate and confidence\n",
    "                        slow_factor = 1.0 - min(1.0, speaking_rate / 8)  # Slower = more hesitation\n",
    "                        direct_metrics_emotions[\"hesitation\"] = 0.6 * slow_factor + 0.4 * (1.0 - volume_factor)\n",
    "                        \n",
    "                        # 4. Engagement: Based on clarity, pitch variation, and balanced speaking rate\n",
    "                        rate_balance = 1.0 - abs(norm_speaking_rate - 0.5) * 2  # Penalize too fast/slow\n",
    "                        direct_metrics_emotions[\"engagement\"] = 0.4 * self.speech_metrics[\"clarity\"] + 0.3 * norm_pitch_var + 0.3 * rate_balance\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in audio metrics: {e}\")\n",
    "\n",
    "            # Use only direct metrics for emotions\n",
    "            for emotion in self.voice_emotions:\n",
    "                self.voice_emotions[emotion] = min(1.0, max(0.0, direct_metrics_emotions[emotion]))\n",
    "\n",
    "            # Notify callback if available (always log an interval)\n",
    "            if self.callback:\n",
    "                self.callback(self.speech_metrics, self.voice_emotions)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio chunk: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae915fd-cc3d-4dc1-83fc-c9c32e1b12a8",
   "metadata": {},
   "source": [
    "#### Emotion Logging GUI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a08427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionLoggingApp:\n",
    "    def __init__(self, window, window_title):\n",
    "        self.window = window\n",
    "        self.window.title(window_title)\n",
    "        self.window.geometry(\"1200x800\")  # Increased height for summary\n",
    "        \n",
    "        # Initialize models\n",
    "        self.init_models()\n",
    "        \n",
    "        # Create the UI elements\n",
    "        self.create_ui()\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.is_running = False\n",
    "        self.stop_event = Event()\n",
    "        self.emotion_logs = []\n",
    "        self.current_emotions = []\n",
    "        self.speech_logs = []\n",
    "        self.logging_start_time = None\n",
    "        self.last_log_time = None\n",
    "        self.transcript_file_path = None\n",
    "        self.stopwatch_active = False\n",
    "        self.elapsed_time = 0\n",
    "        \n",
    "        # Start video capture\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.update()\n",
    "        \n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "        self.window.mainloop()\n",
    "    \n",
    "    def init_models(self):\n",
    "        # MediaPipe setup\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        \n",
    "        name_LSTM_model = 'lstm_model.onnx'\n",
    "        name_resnet_feature_model = 'resnet_feature_extractor.onnx'\n",
    "        \n",
    "        self.session_lstm = ort.InferenceSession(name_LSTM_model, \n",
    "                                            providers=[(\"QNNExecutionProvider\", QNN_PROVIDER_OPTIONS),\n",
    "                                                       \"CPUExecutionProvider\"], sess_options=SESS_OPTS)\n",
    "        \n",
    "        self.session_feature = ort.InferenceSession(name_resnet_feature_model,\n",
    "                                               providers=[(\"QNNExecutionProvider\", QNN_PROVIDER_OPTIONS),\n",
    "                                                          \"CPUExecutionProvider\"], sess_options=SESS_OPTS)\n",
    "        self.lstm_input_name =  self.session_lstm.get_inputs()[0].name\n",
    "        self.feature_extractor_input_name =  self.session_feature.get_inputs()[0].name\n",
    "        # Load ResNet model\n",
    "        print('Facial feature extraction model loaded successfully.')\n",
    "        # Emotion dictionary\n",
    "        self.DICT_EMO = {0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger'}\n",
    "        self.NEGATIVE_EMOTIONS = ['Sadness', 'Fear', 'Disgust', 'Anger']\n",
    "        \n",
    "        # Initialize LSTM features\n",
    "        self.lstm_features = []\n",
    "    \n",
    "    def create_ui(self):\n",
    "        # Main frame\n",
    "        main_frame = ttk.Frame(self.window)\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Left frame for video and transcript\n",
    "        left_frame = ttk.Frame(main_frame)\n",
    "        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Transcript frame at the top of left frame\n",
    "        transcript_frame = ttk.LabelFrame(left_frame, text=\"Presentation Transcript\")\n",
    "        transcript_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Transcript buttons frame\n",
    "        transcript_buttons_frame = ttk.Frame(transcript_frame)\n",
    "        transcript_buttons_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Choose file button\n",
    "        self.choose_file_button = ttk.Button(transcript_buttons_frame, text=\"Choose Transcript File\", command=self.choose_transcript_file)\n",
    "        self.choose_file_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # File name label\n",
    "        self.file_label = ttk.Label(transcript_buttons_frame, text=\"No file selected\")\n",
    "        self.file_label.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Transcript text area\n",
    "        self.transcript_text = scrolledtext.ScrolledText(transcript_frame, wrap=tk.WORD, height=8)\n",
    "        self.transcript_text.pack(fill=tk.X, padx=5, pady=5)\n",
    "        self.transcript_text.insert(tk.END, \"Load a transcript file to view your presentation text here.\")\n",
    "        \n",
    "        # Video frame\n",
    "        self.video_frame = ttk.LabelFrame(left_frame, text=\"Feed: \")\n",
    "        self.video_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Create canvas for video\n",
    "        self.canvas = tk.Canvas(self.video_frame)\n",
    "        self.canvas.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Audio visualizer frame below video\n",
    "        audio_viz_frame = ttk.LabelFrame(left_frame, text=\"Audio Input Level\")\n",
    "        audio_viz_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        viz_content_frame = ttk.Frame(audio_viz_frame, height=120)\n",
    "        viz_content_frame.pack(fill=tk.X)\n",
    "        viz_content_frame.pack_propagate(False)\n",
    "        self.audio_visualizer = AudioVisualizer(viz_content_frame, height=100, width=800)\n",
    "        \n",
    "        # Right frame for controls and summary\n",
    "        right_frame = ttk.Frame(main_frame, width=300)\n",
    "        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, padx=5, pady=5, expand=False)\n",
    "        right_frame.pack_propagate(False)\n",
    "        \n",
    "        # Control frame\n",
    "        control_frame = ttk.LabelFrame(right_frame, text=\"Controls\")\n",
    "        control_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Stopwatch display\n",
    "        stopwatch_frame = ttk.Frame(control_frame)\n",
    "        stopwatch_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(stopwatch_frame, text=\"Elapsed Time: \").pack(side=tk.LEFT)\n",
    "        self.time_label = ttk.Label(stopwatch_frame, text=\"00:00:00\", font=(\"Arial\", 14, \"bold\"))\n",
    "        self.time_label.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Start button\n",
    "        self.start_button = ttk.Button(control_frame, text=\"Start\", command=self.start_logging)\n",
    "        self.start_button.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Stop button\n",
    "        self.stop_button = ttk.Button(control_frame, text=\"Stop\", command=self.stop_logging, state=tk.DISABLED)\n",
    "        self.stop_button.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Reset button\n",
    "        self.reset_button = ttk.Button(control_frame, text=\"Reset\", command=self.reset_logging, state=tk.DISABLED)\n",
    "        self.reset_button.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Status indicator\n",
    "        status_frame = ttk.Frame(control_frame)\n",
    "        status_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(status_frame, text=\"Status: \").pack(side=tk.LEFT)\n",
    "        self.status_label = ttk.Label(status_frame, text=\"Ready\")\n",
    "        self.status_label.pack(side=tk.LEFT)\n",
    "        \n",
    "        # --- Removed Voice Analysis section here ---\n",
    "        \n",
    "        # Add Analyze button above the summary frame\n",
    "        self.analyze_button = ttk.Button(right_frame, text=\"Analyze\", command=self.display_summary, state=tk.DISABLED)\n",
    "        self.analyze_button.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Summary frame (now taller)\n",
    "        summary_frame = ttk.LabelFrame(right_frame, text=\"Analysis Summary\")\n",
    "        summary_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Create scrolled text for summary (increased height)\n",
    "        self.summary_text = scrolledtext.ScrolledText(summary_frame, wrap=tk.WORD, width=30, height=40)\n",
    "        self.summary_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        self.summary_text.config(state=tk.DISABLED)\n",
    "        \n",
    "        # Initialize audio processor (no update_voice_metrics callback)\n",
    "        # self.audio_processor = AudioProcessor(self.audio_visualizer, None)\n",
    "        # Initialize audio processor (now with update_voice_metrics callback)\n",
    "        self.audio_processor = AudioProcessor(self.audio_visualizer, self.update_voice_metrics)\n",
    "    \n",
    "    def update_voice_metrics(self, metrics, emotions):\n",
    "        \"\"\"Callback to log audio metrics/emotions every interval.\"\"\"\n",
    "        # Only log if logging is active\n",
    "        if self.is_running:\n",
    "            log_entry = {\n",
    "                \"timestamp\": int(time.time() - self.logging_start_time) if self.logging_start_time else 0,\n",
    "                \"metrics\": metrics.copy(),\n",
    "                \"emotions\": emotions.copy()\n",
    "            }\n",
    "            self.speech_logs.append(log_entry)\n",
    "    \n",
    "    # --- Removed update_voice_metrics method ---\n",
    "    \n",
    "    def choose_transcript_file(self):\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select Transcript File\",\n",
    "            filetypes=[(\"Text files\", \"*.txt\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            self.transcript_file_path = file_path\n",
    "            filename = os.path.basename(file_path)\n",
    "            self.file_label.config(text=f\"Selected: {filename}\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = file.read()\n",
    "                    self.transcript_text.delete(1.0, tk.END)\n",
    "                    self.transcript_text.insert(tk.END, content)\n",
    "            except Exception as e:\n",
    "                self.transcript_text.delete(1.0, tk.END)\n",
    "                self.transcript_text.insert(tk.END, f\"Error reading file: {str(e)}\")\n",
    "    \n",
    "    def update_stopwatch(self):\n",
    "        if self.stopwatch_active:\n",
    "            current_time = time.time()\n",
    "            self.elapsed_time = current_time - self.logging_start_time\n",
    "            \n",
    "            # Format time as HH:MM:SS\n",
    "            hours, remainder = divmod(int(self.elapsed_time), 3600)\n",
    "            minutes, seconds = divmod(remainder, 60)\n",
    "            time_str = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "            self.time_label.config(text=time_str)\n",
    "            \n",
    "            # Update every 1 second\n",
    "            self.window.after(1000, self.update_stopwatch)\n",
    "    \n",
    "    def update(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            # Process frame if logging is active\n",
    "            if self.is_running:\n",
    "                self.process_frame(frame)\n",
    "            \n",
    "            # Convert to RGB for display\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = PIL.Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # Resize to fit canvas\n",
    "            canvas_width = self.canvas.winfo_width()\n",
    "            canvas_height = self.canvas.winfo_height()\n",
    "            \n",
    "            if canvas_width > 1 and canvas_height > 1:\n",
    "                ratio = min(canvas_width/img.width, canvas_height/img.height)\n",
    "                new_width = int(img.width * ratio)\n",
    "                new_height = int(img.height * ratio)\n",
    "                img = img.resize((new_width, new_height), PIL.Image.Resampling.LANCZOS)\n",
    "            \n",
    "            self.photo = PIL.ImageTk.PhotoImage(image=img)\n",
    "            self.canvas.create_image(canvas_width//2, canvas_height//2, image=self.photo, anchor=tk.CENTER)\n",
    "        \n",
    "        if not self.stop_event.is_set():\n",
    "            self.window.after(10, self.update)\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Initialize the 5-second logging interval\n",
    "        if self.last_log_time is None:\n",
    "            self.last_log_time = current_time\n",
    "        \n",
    "        # Process frame for emotion detection\n",
    "        frame_copy = frame.copy()\n",
    "        frame_copy.flags.writeable = False\n",
    "        frame_copy = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as face_mesh:\n",
    "            \n",
    "            results = face_mesh.process(frame_copy)\n",
    "\n",
    "            results = face_mesh.process(frame_copy)\n",
    "            \n",
    "            if results.multi_face_landmarks:\n",
    "                for fl in results.multi_face_landmarks:\n",
    "                    h, w, _ = frame_copy.shape\n",
    "                    startX, startY, endX, endY = get_box(fl, w, h)\n",
    "                    \n",
    "                    # Extract face\n",
    "                    cur_face = frame_copy[startY:endY, startX:endX]\n",
    "                    if cur_face.size == 0:  # Skip if face not properly detected\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Process with model\n",
    "                        cur_face = pth_processing_np(Image.fromarray(cur_face))\n",
    "                        #print(f\"Processing face with shape: {cur_face.shape}\")\n",
    "                        features = self.session_feature.run(None, {self.feature_extractor_input_name: cur_face})[0]\n",
    "                        features = np.maximum(features, 0).reshape((-1,))  # Apply ReLU activation\n",
    "                        #print(f\"Extracted features shape: {features.shape}\")\n",
    "                        # Update LSTM features\n",
    "                        if len(self.lstm_features) == 0:\n",
    "                            self.lstm_features = [features] * 10\n",
    "                        else:\n",
    "                            self.lstm_features = self.lstm_features[1:] + [features]\n",
    "                        \n",
    "                        lstm_f = np.array(self.lstm_features)\n",
    "                        lstm_f = np.expand_dims(lstm_f, axis=0)  # Add\n",
    "                        #print(f\"LSTM input shape: {lstm_f.shape}\")\n",
    "                        output = self.session_lstm.run(None, {self.lstm_input_name: lstm_f})[0][0]\n",
    "                        #print(f\"LSTM output shape: {output.shape}\")\n",
    "                        # Get emotion label\n",
    "                        cl = np.argmax(output)\n",
    "                        #print(f\"Predicted class index: {cl}\")\n",
    "                        emotion = self.DICT_EMO[cl]\n",
    "                        #print(f\"Detected emotion: {emotion} with confidence {output[cl]:.2f}\")\n",
    "                        confidence = output[cl]\n",
    "                        \n",
    "                        # Add to current emotions\n",
    "                        self.current_emotions.append(emotion)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing face: {e}\")\n",
    "            \n",
    "        # Log emotions every 5 seconds\n",
    "        if current_time - self.last_log_time >= 5 and self.current_emotions:\n",
    "            self.log_emotions()\n",
    "            self.last_log_time = current_time\n",
    "    \n",
    "    def log_emotions(self):\n",
    "        if not self.current_emotions:\n",
    "            return\n",
    "        \n",
    "        # Count occurrences of each emotion\n",
    "        emotion_counts = Counter(self.current_emotions)\n",
    "        dominant_emotion = emotion_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # Check for negative emotions\n",
    "        negative_emotions = [emotion for emotion in self.current_emotions if emotion in self.NEGATIVE_EMOTIONS]\n",
    "        has_negative = len(negative_emotions) > 0\n",
    "        \n",
    "        # Create log entry\n",
    "        elapsed_time = int(time.time() - self.logging_start_time)\n",
    "        timestamp = f\"{elapsed_time//60:02d}:{elapsed_time%60:02d}\"\n",
    "        \n",
    "        log_entry = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"dominant_emotion\": dominant_emotion,\n",
    "            \"counts\": dict(emotion_counts),\n",
    "            \"has_negative\": has_negative\n",
    "        }\n",
    "        \n",
    "        self.emotion_logs.append(log_entry)\n",
    "        \n",
    "        # Clear current emotions for next interval\n",
    "        self.current_emotions = []\n",
    "    \n",
    "    def start_logging(self):\n",
    "        self.is_running = True\n",
    "        self.logging_start_time = time.time()\n",
    "        self.last_log_time = None\n",
    "        self.emotion_logs = []\n",
    "        self.speech_logs = []\n",
    "        self.current_emotions = []\n",
    "        \n",
    "        # Start audio processing\n",
    "        self.audio_processor.start_recording()\n",
    "        \n",
    "        # Start stopwatch\n",
    "        self.stopwatch_active = True\n",
    "        self.update_stopwatch()\n",
    "        \n",
    "        # Update UI\n",
    "        self.status_label.config(text=\"Logging\")\n",
    "        self.start_button.config(state=tk.DISABLED)\n",
    "        self.stop_button.config(state=tk.NORMAL)\n",
    "        self.reset_button.config(state=tk.DISABLED)\n",
    "        self.analyze_button.config(state=tk.DISABLED)  # Disable analyze button during recording\n",
    "        self.choose_file_button.config(state=tk.DISABLED)  # Disable file selection during recording\n",
    "        \n",
    "        # Clear summary\n",
    "        self.summary_text.config(state=tk.NORMAL)\n",
    "        self.summary_text.delete(1.0, tk.END)\n",
    "        self.summary_text.insert(tk.END, \"Logging emotions and voice metrics...\\n\")\n",
    "        self.summary_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    def stop_logging(self):\n",
    "        if self.is_running:\n",
    "            self.is_running = False\n",
    "            self.stopwatch_active = False\n",
    "            \n",
    "            # Stop audio recording and process final audio\n",
    "            self.audio_processor.stop_recording()\n",
    "            \n",
    "            # Log any remaining emotions\n",
    "            if self.current_emotions:\n",
    "                self.log_emotions()\n",
    "            \n",
    "            # Update UI\n",
    "            self.status_label.config(text=\"Ready\")\n",
    "            self.start_button.config(state=tk.DISABLED)\n",
    "            self.stop_button.config(state=tk.DISABLED)\n",
    "            self.reset_button.config(state=tk.NORMAL)\n",
    "            self.analyze_button.config(state=tk.NORMAL)  # Enable the analyze button\n",
    "            self.choose_file_button.config(state=tk.NORMAL)  # Re-enable file selection\n",
    "            \n",
    "            # No longer automatically display summary - user must click Analyze button\n",
    "            # self.display_summary()\n",
    "            \n",
    "            # Save video summary to text file\n",
    "            self.save_video_summary()\n",
    "            # Save audio summary to text file\n",
    "            self.save_audio_summary()\n",
    "    \n",
    "    def save_video_summary(self):\n",
    "        \"\"\"Save the video summary to a text file.\"\"\"\n",
    "        try:\n",
    "            with open(\"video_logging.txt\", \"w\") as file:\n",
    "                file.write(\"== Emotion Logging Summary ==\\n\")\n",
    "                \n",
    "                # Format final time\n",
    "                hours, remainder = divmod(int(self.elapsed_time), 3600)\n",
    "                minutes, seconds = divmod(remainder, 60)\n",
    "                time_str = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "                file.write(f\"Total Duration: {time_str}\\n\")\n",
    "                \n",
    "                # Log 5-second intervals\n",
    "                file.write(\"=== 5-second Intervals ===\\n\")\n",
    "                for log in self.emotion_logs:\n",
    "                    file.write(f\"[{log['timestamp']}] Dominant: {log['dominant_emotion']}\\n\")\n",
    "                    #if log['has_negative']:\n",
    "                        #file.write(\"⚠️ Negative emotions detected!\\n\")\n",
    "                    for emotion, count in log['counts'].items():\n",
    "                        percentage = count / sum(log['counts'].values()) * 100\n",
    "                        file.write(f\"* {emotion}: {percentage:.1f}%\\n\")\n",
    "                \n",
    "                # Overall summary\n",
    "                file.write(\"=== Overall Summary ===\\n\")\n",
    "                if self.emotion_logs:\n",
    "                    dominant_counts = Counter([log['dominant_emotion'] for log in self.emotion_logs])\n",
    "                    most_common = dominant_counts.most_common()\n",
    "                    file.write(\"Most frequent emotions:\\n\")\n",
    "                    for emotion, count in most_common:\n",
    "                        percentage = count / len(self.emotion_logs) * 100\n",
    "                        file.write(f\"* {emotion}: {percentage:.1f}%\\n\")\n",
    "                    \n",
    "                    negative_intervals = sum(1 for log in self.emotion_logs if log['has_negative'])\n",
    "                    neg_percentage = negative_intervals / len(self.emotion_logs) * 100\n",
    "                    #file.write(f\"⚠️ Negative emotions detected in {negative_intervals} intervals ({neg_percentage:.1f}%)\\n\")\n",
    "            # --- Write top 2 dominant emotions to video_overall_log.txt ---\n",
    "            if self.emotion_logs:\n",
    "                dominant_counts = Counter([log['dominant_emotion'] for log in self.emotion_logs])\n",
    "                top2 = dominant_counts.most_common(2)\n",
    "                with open(\"video_overall_log.txt\", \"w\") as f:\n",
    "                    for emotion, count in top2:\n",
    "                        percentage = count / len(self.emotion_logs) * 100\n",
    "                        f.write(f\"{emotion}: {percentage:.1f}%\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving video summary: {e}\")\n",
    "    \n",
    "    def save_audio_summary(self):\n",
    "        \"\"\"Save the audio summary to a text file.\"\"\"\n",
    "        try:\n",
    "            with open(\"audio_logging.txt\", \"w\") as f:\n",
    "                f.write(\"== voice attribute Logging Summary ==\\n\")\n",
    "                # total duration\n",
    "                hours, rem = divmod(int(self.elapsed_time), 3600)\n",
    "                mins, secs = divmod(rem, 60)\n",
    "                f.write(f\"Total Duration: {hours:02d}:{mins:02d}:{secs:02d}\\n\")\n",
    "                f.write(\"=== 5-second Intervals ===\\n\")\n",
    "                from collections import Counter\n",
    "                doms = []\n",
    "                # iterate through each logged audio entry\n",
    "                for idx, log in enumerate(self.speech_logs, start=1):\n",
    "                    dom_emotion = max(log['emotions'], key=log['emotions'].get)\n",
    "                    doms.append(dom_emotion)\n",
    "                    f.write(f\"[Interval {idx}] Dominant: {dom_emotion}\\n\\n\")\n",
    "                    # write each emotion and metric\n",
    "                    # for k, v in {**log['emotions'], **log['metrics']}.items():\n",
    "                    #     f.write(f\"{k}: {v:.1f}\\n\")\n",
    "                    # f.write(\"\\n\")\n",
    "                    # Only write relevant metrics/emotions\n",
    "                    for k, v in {**log['emotions'], **log['metrics']}.items():\n",
    "                        if k in [\"samples_processed\", \"volume_db\"]:\n",
    "                            continue  # Skip these keys\n",
    "                        if k == \"volume\":\n",
    "                            scaled_volume = v * 100\n",
    "                            f.write(f\"volume: {scaled_volume:.1f}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{k}: {v:.1f}\\n\")\n",
    "                # overall summary\n",
    "                # f.write(\"=== Overall Summary ===\\n\")\n",
    "                # f.write(\"Most frequent emotions:\\n\")\n",
    "                # counts = Counter(doms)\n",
    "                # for em, cnt in counts.items():\n",
    "                #     pct = cnt / len(doms) * 100 if doms else 0\n",
    "                #     f.write(f\"{em}: {pct:.1f}%\\n\")\n",
    "            # --- Write top 2 dominant emotions to audio_overall_log.txt ---\n",
    "            if self.speech_logs:\n",
    "                doms = []\n",
    "                for log in self.speech_logs:\n",
    "                    dom_emotion = max(log['emotions'], key=log['emotions'].get)\n",
    "                    doms.append(dom_emotion)\n",
    "                from collections import Counter\n",
    "                counts = Counter(doms)\n",
    "                top2 = counts.most_common(2)\n",
    "                with open(\"audio_overall_log.txt\", \"w\") as f2:\n",
    "                    for emotion, count in top2:\n",
    "                        percentage = count / len(doms) * 100 if doms else 0\n",
    "                        f2.write(f\"{emotion}: {percentage:.1f}%\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving audio summary: {e}\")\n",
    "    \n",
    "    def reset_logging(self):\n",
    "        # Reset all variables to initial state\n",
    "        self.emotion_logs = []\n",
    "        self.speech_logs = []\n",
    "        self.current_emotions = []\n",
    "        self.logging_start_time = None\n",
    "        self.last_log_time = None\n",
    "        self.elapsed_time = 0\n",
    "        self.lstm_features = []\n",
    "        \n",
    "        # Overwrite the video summary file\n",
    "        try:\n",
    "            with open(\"video_logging.txt\", \"w\") as file:\n",
    "                file.write(\"Ready for new logging session.\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error resetting video summary file: {e}\")\n",
    "        \n",
    "        # Reset UI\n",
    "        self.time_label.config(text=\"00:00:00\")\n",
    "        self.status_label.config(text=\"Ready\")\n",
    "        self.start_button.config(state=tk.NORMAL)\n",
    "        self.stop_button.config(state=tk.DISABLED)\n",
    "        self.reset_button.config(state=tk.DISABLED)\n",
    "        self.analyze_button.config(state=tk.DISABLED)  # Disable analyze button\n",
    "        \n",
    "        # Reset voice metrics\n",
    "        # for metric in [\"confidence\", \"enthusiasm\", \"clarity\", \"speaking rate\"]:\n",
    "        #     self.voice_labels[metric].config(text=\"N/A\")\n",
    "        #     self.voice_labels[f\"{metric}_bar\"][\"value\"] = 0\n",
    "        \n",
    "        # Clear summary\n",
    "        self.summary_text.config(state=tk.NORMAL)\n",
    "        self.summary_text.delete(1.0, tk.END)\n",
    "        self.summary_text.insert(tk.END, \"Ready to start a new session.\\n\")\n",
    "        self.summary_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    def display_summary(self):\n",
    "        def run_report_and_display():\n",
    "            # Show progress dialog\n",
    "            progress_win = tk.Toplevel(self.window)\n",
    "            progress_win.title(\"Generating Report\")\n",
    "            progress_win.geometry(\"300x80\")\n",
    "            progress_win.transient(self.window)\n",
    "            progress_win.grab_set()\n",
    "            label = ttk.Label(progress_win, text=\"Generating report, please wait...\")\n",
    "            label.pack(pady=10)\n",
    "            pb = ttk.Progressbar(progress_win, mode=\"indeterminate\")\n",
    "            pb.pack(fill=tk.X, padx=20, pady=5)\n",
    "            pb.start(10)\n",
    "            \n",
    "            def worker():\n",
    "                try:\n",
    "                    subprocess.run(\n",
    "                        [\"python\", \"generate_report.py\"],\n",
    "                        check=True,\n",
    "                        cwd=os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error running generate_report.py: {e}\")\n",
    "                # Schedule finish_report to run in the main thread\n",
    "                self.window.after(0, finish_report)\n",
    "\n",
    "            def finish_report():\n",
    "                pb.stop()\n",
    "                progress_win.destroy()\n",
    "                # Display the output from presenattion_report.txt\n",
    "                report_path = \"presentation_feedback.txt\"\n",
    "                self.summary_text.config(state=tk.NORMAL)\n",
    "                self.summary_text.delete(1.0, tk.END)\n",
    "                try:\n",
    "                    with open(report_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        report_content = f.read()\n",
    "                        self.summary_text.insert(tk.END, report_content)\n",
    "                except Exception as e:\n",
    "                    self.summary_text.insert(tk.END, f\"Could not load report: {e}\")\n",
    "                self.summary_text.config(state=tk.DISABLED)\n",
    "\n",
    "            \n",
    "\n",
    "            threading.Thread(target=worker, daemon=True).start()\n",
    "\n",
    "        # Run the report generation and display in the main thread\n",
    "        self.window.after(0, run_report_and_display)\n",
    "        \n",
    "    def on_closing(self):\n",
    "        self.stop_event.set()\n",
    "        if hasattr(self, 'audio_processor'):\n",
    "            self.audio_processor.stop_recording()\n",
    "        if self.cap and self.cap.isOpened():\n",
    "            self.cap.release()\n",
    "        self.window.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b94e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facial feature extraction model loaded successfully.\n",
      "Audio recording started\n",
      "Audio recording stopped\n",
      "Error running generate_report.py: Command '['python', 'generate_report.py']' returned non-zero exit status 1.\n",
      "Error running generate_report.py: Command '['python', 'generate_report.py']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "# Run the application\n",
    "root = tk.Tk()\n",
    "app = EmotionLoggingApp(root, \"Presentation Rehearsal Voice & Emotion Tracker\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
