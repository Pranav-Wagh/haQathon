{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e1b0206-2912-4385-97b9-5948ed70dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp #face detector\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import tkinter for GUI\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, scrolledtext, filedialog\n",
    "import PIL.Image, PIL.ImageTk\n",
    "from threading import Thread, Event\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0907155",
   "metadata": {},
   "source": [
    "#### Model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67038e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.99)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding='same', bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.99)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion, eps=0.001, momentum=0.99)\n",
    "        \n",
    "        self.i_downsample = i_downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        \n",
    "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        \n",
    "        #downsample if needed\n",
    "        if self.i_downsample is not None:\n",
    "            identity = self.i_downsample(identity)\n",
    "        #add identity\n",
    "        x+=identity\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Conv2dSame(torch.nn.Conv2d):\n",
    "\n",
    "    def calc_same_pad(self, i: int, k: int, s: int, d: int) -> int:\n",
    "        return max((math.ceil(i / s) - 1) * s + (k - 1) * d + 1 - i, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ih, iw = x.size()[-2:]\n",
    "\n",
    "        pad_h = self.calc_same_pad(i=ih, k=self.kernel_size[0], s=self.stride[0], d=self.dilation[0])\n",
    "        pad_w = self.calc_same_pad(i=iw, k=self.kernel_size[1], s=self.stride[1], d=self.dilation[1])\n",
    "\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(\n",
    "                x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n",
    "            )\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            self.weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv_layer_s2_same = Conv2dSame(num_channels, 64, 7, stride=2, groups=1, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64, eps=0.001, momentum=0.99)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2)\n",
    "        \n",
    "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64, stride=1)\n",
    "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
    "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
    "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(512*ResBlock.expansion, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.relu(self.batch_norm1(self.conv_layer_s2_same(x)))\n",
    "        x = self.max_pool(x)\n",
    "        # print(x.shape)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
    "        ii_downsample = None\n",
    "        layers = []\n",
    "        \n",
    "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
    "            ii_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride, bias=False, padding=0),\n",
    "                nn.BatchNorm2d(planes*ResBlock.expansion, eps=0.001, momentum=0.99)\n",
    "            )\n",
    "            \n",
    "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
    "        self.in_channels = planes*ResBlock.expansion\n",
    "        \n",
    "        for i in range(blocks-1):\n",
    "            layers.append(ResBlock(self.in_channels, planes))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "def ResNet50(num_classes, channels=3):\n",
    "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n",
    "\n",
    "\n",
    "class LSTMPyTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMPyTorch, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=512, hidden_size=512, batch_first=True, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(input_size=512, hidden_size=256, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(256, 7)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)        \n",
    "        x = self.fc(x[:, -1, :])\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcf9fa-a7cc-4d4c-b723-6d7efd49b94b",
   "metadata": {},
   "source": [
    "#### Sub functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0fc324-98a8-4efc-bb11-4bec8a015790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pth_processing(fp):\n",
    "    class PreprocessInput(torch.nn.Module):\n",
    "        def init(self):\n",
    "            super(PreprocessInput, self).init()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.to(torch.float32)\n",
    "            x = torch.flip(x, dims=(0,))\n",
    "            x[0, :, :] -= 91.4953\n",
    "            x[1, :, :] -= 103.8827\n",
    "            x[2, :, :] -= 131.0912\n",
    "            return x\n",
    "\n",
    "    def get_img_torch(img):\n",
    "        \n",
    "        ttransform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            PreprocessInput()\n",
    "        ])\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        img = ttransform(img)\n",
    "        img = torch.unsqueeze(img, 0)\n",
    "        return img\n",
    "    return get_img_torch(fp)\n",
    "\n",
    "def tf_processing(fp):\n",
    "    def preprocess_input(x):\n",
    "        x_temp = np.copy(x)\n",
    "        x_temp = x_temp[..., ::-1]\n",
    "        x_temp[..., 0] -= 91.4953\n",
    "        x_temp[..., 1] -= 103.8827\n",
    "        x_temp[..., 2] -= 131.0912\n",
    "        return x_temp\n",
    "\n",
    "    def get_img_tf(img):\n",
    "        img = cv2.resize(img, (224,224), interpolation=cv2.INTER_NEAREST)\n",
    "        img = tf.keras.utils.img_to_array(img)\n",
    "        img = preprocess_input(img)\n",
    "        img = np.array([img])\n",
    "        return img\n",
    "\n",
    "    return get_img_tf(fp)\n",
    "\n",
    "def norm_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    \n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    \n",
    "    return x_px, y_px\n",
    "\n",
    "def get_box(fl, w, h):\n",
    "    idx_to_coors = {}\n",
    "    for idx, landmark in enumerate(fl.landmark):\n",
    "        landmark_px = norm_coordinates(landmark.x, landmark.y, w, h)\n",
    "\n",
    "        if landmark_px:\n",
    "            idx_to_coors[idx] = landmark_px\n",
    "\n",
    "    x_min = np.min(np.asarray(list(idx_to_coors.values()))[:,0])\n",
    "    y_min = np.min(np.asarray(list(idx_to_coors.values()))[:,1])\n",
    "    endX = np.max(np.asarray(list(idx_to_coors.values()))[:,0])\n",
    "    endY = np.max(np.asarray(list(idx_to_coors.values()))[:,1])\n",
    "\n",
    "    (startX, startY) = (max(0, x_min), max(0, y_min))\n",
    "    (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "    \n",
    "    return startX, startY, endX, endY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae915fd-cc3d-4dc1-83fc-c9c32e1b12a8",
   "metadata": {},
   "source": [
    "#### Emotion Logging GUI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05ed967-a30e-47f5-96ed-b32bab0c6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionLoggingApp:\n",
    "    def __init__(self, window, window_title):\n",
    "        self.window = window\n",
    "        self.window.title(window_title)\n",
    "        self.window.geometry(\"1200x700\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self.init_models()\n",
    "        \n",
    "        # Create the UI elements\n",
    "        self.create_ui()\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.is_running = False\n",
    "        self.stop_event = Event()\n",
    "        self.emotion_logs = []\n",
    "        self.current_emotions = []\n",
    "        self.logging_start_time = None\n",
    "        self.last_log_time = None\n",
    "        self.transcript_file_path = None\n",
    "        self.stopwatch_active = False\n",
    "        self.elapsed_time = 0\n",
    "        \n",
    "        # Start video capture\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.update()\n",
    "        \n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "        self.window.mainloop()\n",
    "    \n",
    "    def init_models(self):\n",
    "        # MediaPipe setup\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        \n",
    "        # Model settings\n",
    "        name_backbone_model = 'FER_static_ResNet50_AffectNet.pt'\n",
    "        name_LSTM_model = 'Aff-Wild2'\n",
    "        \n",
    "        # Load ResNet model\n",
    "        self.pth_backbone_model = ResNet50(7, channels=3)\n",
    "        self.pth_backbone_model.load_state_dict(torch.load(name_backbone_model))\n",
    "        self.pth_backbone_model.eval()\n",
    "        \n",
    "        # Load LSTM model\n",
    "        self.pth_LSTM_model = LSTMPyTorch()\n",
    "        self.pth_LSTM_model.load_state_dict(torch.load(f'FER_dinamic_LSTM_{name_LSTM_model}.pt'))\n",
    "        self.pth_LSTM_model.eval()\n",
    "        \n",
    "        # Emotion dictionary\n",
    "        self.DICT_EMO = {0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger'}\n",
    "        self.NEGATIVE_EMOTIONS = ['Sadness', 'Fear', 'Disgust', 'Anger']\n",
    "        \n",
    "        # Initialize LSTM features\n",
    "        self.lstm_features = []\n",
    "    \n",
    "    def create_ui(self):\n",
    "        # Main frame\n",
    "        main_frame = ttk.Frame(self.window)\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Left frame for video and transcript\n",
    "        left_frame = ttk.Frame(main_frame)\n",
    "        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Transcript frame at the top of left frame\n",
    "        transcript_frame = ttk.LabelFrame(left_frame, text=\"Presentation Transcript\")\n",
    "        transcript_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Transcript buttons frame\n",
    "        transcript_buttons_frame = ttk.Frame(transcript_frame)\n",
    "        transcript_buttons_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Choose file button\n",
    "        self.choose_file_button = ttk.Button(transcript_buttons_frame, text=\"Choose Transcript File\", command=self.choose_transcript_file)\n",
    "        self.choose_file_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # File name label\n",
    "        self.file_label = ttk.Label(transcript_buttons_frame, text=\"No file selected\")\n",
    "        self.file_label.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Transcript text area\n",
    "        self.transcript_text = scrolledtext.ScrolledText(transcript_frame, wrap=tk.WORD, height=8)\n",
    "        self.transcript_text.pack(fill=tk.X, padx=5, pady=5)\n",
    "        self.transcript_text.insert(tk.END, \"Load a transcript file to view your presentation text here.\")\n",
    "        \n",
    "        # Video frame\n",
    "        self.video_frame = ttk.LabelFrame(left_frame, text=\"Webcam Feed\")\n",
    "        self.video_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Create canvas for video\n",
    "        self.canvas = tk.Canvas(self.video_frame)\n",
    "        self.canvas.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Right frame for controls and summary\n",
    "        right_frame = ttk.Frame(main_frame, width=300)\n",
    "        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, padx=5, pady=5, expand=False)\n",
    "        right_frame.pack_propagate(False)  # Prevent the frame from shrinking to fit its contents\n",
    "        \n",
    "        # Control frame\n",
    "        control_frame = ttk.LabelFrame(right_frame, text=\"Controls\")\n",
    "        control_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Stopwatch display\n",
    "        stopwatch_frame = ttk.Frame(control_frame)\n",
    "        stopwatch_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(stopwatch_frame, text=\"Elapsed Time: \").pack(side=tk.LEFT)\n",
    "        self.time_label = ttk.Label(stopwatch_frame, text=\"00:00:00\", font=(\"Arial\", 14, \"bold\"))\n",
    "        self.time_label.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Start button\n",
    "        self.start_button = ttk.Button(control_frame, text=\"Start Logging\", command=self.start_logging)\n",
    "        self.start_button.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Stop button\n",
    "        self.stop_button = ttk.Button(control_frame, text=\"Stop Logging\", command=self.stop_logging, state=tk.DISABLED)\n",
    "        self.stop_button.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Reset button\n",
    "        self.reset_button = ttk.Button(control_frame, text=\"Reset\", command=self.reset_logging, state=tk.DISABLED)\n",
    "        self.reset_button.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Status indicator\n",
    "        status_frame = ttk.Frame(control_frame)\n",
    "        status_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(status_frame, text=\"Status: \").pack(side=tk.LEFT)\n",
    "        self.status_label = ttk.Label(status_frame, text=\"Ready\")\n",
    "        self.status_label.pack(side=tk.LEFT)\n",
    "        \n",
    "        # Summary frame\n",
    "        summary_frame = ttk.LabelFrame(right_frame, text=\"Emotion Summary\")\n",
    "        summary_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Create scrolled text for summary\n",
    "        self.summary_text = scrolledtext.ScrolledText(summary_frame, wrap=tk.WORD, width=30, height=20)\n",
    "        self.summary_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        self.summary_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    def choose_transcript_file(self):\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select Transcript File\",\n",
    "            filetypes=[(\"Text files\", \"*.txt\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            self.transcript_file_path = file_path\n",
    "            filename = os.path.basename(file_path)\n",
    "            self.file_label.config(text=f\"Selected: {filename}\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = file.read()\n",
    "                    self.transcript_text.delete(1.0, tk.END)\n",
    "                    self.transcript_text.insert(tk.END, content)\n",
    "            except Exception as e:\n",
    "                self.transcript_text.delete(1.0, tk.END)\n",
    "                self.transcript_text.insert(tk.END, f\"Error reading file: {str(e)}\")\n",
    "    \n",
    "    def update_stopwatch(self):\n",
    "        if self.stopwatch_active:\n",
    "            current_time = time.time()\n",
    "            self.elapsed_time = current_time - self.logging_start_time\n",
    "            \n",
    "            # Format time as HH:MM:SS\n",
    "            hours, remainder = divmod(int(self.elapsed_time), 3600)\n",
    "            minutes, seconds = divmod(remainder, 60)\n",
    "            time_str = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "            self.time_label.config(text=time_str)\n",
    "            \n",
    "            # Update every 1 second\n",
    "            self.window.after(1000, self.update_stopwatch)\n",
    "    \n",
    "    def update(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            # Process frame if logging is active\n",
    "            if self.is_running:\n",
    "                self.process_frame(frame)\n",
    "            \n",
    "            # Convert to RGB for display\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = PIL.Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # Resize to fit canvas\n",
    "            canvas_width = self.canvas.winfo_width()\n",
    "            canvas_height = self.canvas.winfo_height()\n",
    "            \n",
    "            if canvas_width > 1 and canvas_height > 1:\n",
    "                ratio = min(canvas_width/img.width, canvas_height/img.height)\n",
    "                new_width = int(img.width * ratio)\n",
    "                new_height = int(img.height * ratio)\n",
    "                img = img.resize((new_width, new_height), PIL.Image.Resampling.LANCZOS)\n",
    "            \n",
    "            self.photo = PIL.ImageTk.PhotoImage(image=img)\n",
    "            self.canvas.create_image(canvas_width//2, canvas_height//2, image=self.photo, anchor=tk.CENTER)\n",
    "        \n",
    "        if not self.stop_event.is_set():\n",
    "            self.window.after(10, self.update)\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Initialize the 10-second logging interval\n",
    "        if self.last_log_time is None:\n",
    "            self.last_log_time = current_time\n",
    "        \n",
    "        # Process frame for emotion detection\n",
    "        frame_copy = frame.copy()\n",
    "        frame_copy.flags.writeable = False\n",
    "        frame_copy = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as face_mesh:\n",
    "            \n",
    "            results = face_mesh.process(frame_copy)\n",
    "            \n",
    "            if results.multi_face_landmarks:\n",
    "                for fl in results.multi_face_landmarks:\n",
    "                    h, w, _ = frame_copy.shape\n",
    "                    startX, startY, endX, endY = get_box(fl, w, h)\n",
    "                    \n",
    "                    # Extract face\n",
    "                    cur_face = frame_copy[startY:endY, startX:endX]\n",
    "                    if cur_face.size == 0:  # Skip if face not properly detected\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Process with model\n",
    "                        cur_face = pth_processing(Image.fromarray(cur_face))\n",
    "                        features = torch.nn.functional.relu(self.pth_backbone_model.extract_features(cur_face)).detach().numpy()\n",
    "                        \n",
    "                        # Update LSTM features\n",
    "                        if len(self.lstm_features) == 0:\n",
    "                            self.lstm_features = [features] * 10\n",
    "                        else:\n",
    "                            self.lstm_features = self.lstm_features[1:] + [features]\n",
    "                        \n",
    "                        lstm_f = torch.from_numpy(np.vstack(self.lstm_features))\n",
    "                        lstm_f = torch.unsqueeze(lstm_f, 0)\n",
    "                        output = self.pth_LSTM_model(lstm_f).detach().numpy()\n",
    "                        \n",
    "                        # Get emotion label\n",
    "                        cl = np.argmax(output)\n",
    "                        emotion = self.DICT_EMO[cl]\n",
    "                        confidence = output[0][cl]\n",
    "                        \n",
    "                        # Add to current emotions\n",
    "                        self.current_emotions.append(emotion)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing face: {e}\")\n",
    "        \n",
    "        # Log emotions every 10 seconds\n",
    "        if current_time - self.last_log_time >= 10 and self.current_emotions:\n",
    "            self.log_emotions()\n",
    "            self.last_log_time = current_time\n",
    "    \n",
    "    def log_emotions(self):\n",
    "        if not self.current_emotions:\n",
    "            return\n",
    "        \n",
    "        # Count occurrences of each emotion\n",
    "        emotion_counts = Counter(self.current_emotions)\n",
    "        dominant_emotion = emotion_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # Check for negative emotions\n",
    "        negative_emotions = [emotion for emotion in self.current_emotions if emotion in self.NEGATIVE_EMOTIONS]\n",
    "        has_negative = len(negative_emotions) > 0\n",
    "        \n",
    "        # Create log entry\n",
    "        elapsed_time = int(time.time() - self.logging_start_time)\n",
    "        timestamp = f\"{elapsed_time//60:02d}:{elapsed_time%60:02d}\"\n",
    "        \n",
    "        log_entry = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"dominant_emotion\": dominant_emotion,\n",
    "            \"counts\": dict(emotion_counts),\n",
    "            \"has_negative\": has_negative\n",
    "        }\n",
    "        \n",
    "        self.emotion_logs.append(log_entry)\n",
    "        \n",
    "        # Clear current emotions for next interval\n",
    "        self.current_emotions = []\n",
    "    \n",
    "    def start_logging(self):\n",
    "        self.is_running = True\n",
    "        self.logging_start_time = time.time()\n",
    "        self.last_log_time = None\n",
    "        self.emotion_logs = []\n",
    "        self.current_emotions = []\n",
    "        \n",
    "        # Start stopwatch\n",
    "        self.stopwatch_active = True\n",
    "        self.update_stopwatch()\n",
    "        \n",
    "        # Update UI\n",
    "        self.status_label.config(text=\"Logging\")\n",
    "        self.start_button.config(state=tk.DISABLED)\n",
    "        self.stop_button.config(state=tk.NORMAL)\n",
    "        self.reset_button.config(state=tk.DISABLED)\n",
    "        self.choose_file_button.config(state=tk.DISABLED)  # Disable file selection during recording\n",
    "        \n",
    "        # Clear summary\n",
    "        self.summary_text.config(state=tk.NORMAL)\n",
    "        self.summary_text.delete(1.0, tk.END)\n",
    "        self.summary_text.insert(tk.END, \"Logging emotions...\\n\")\n",
    "        self.summary_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    def stop_logging(self):\n",
    "        if self.is_running:\n",
    "            self.is_running = False\n",
    "            self.stopwatch_active = False\n",
    "            \n",
    "            # Log any remaining emotions\n",
    "            if self.current_emotions:\n",
    "                self.log_emotions()\n",
    "            \n",
    "            # Update UI\n",
    "            self.status_label.config(text=\"Ready\")\n",
    "            self.start_button.config(state=tk.DISABLED)\n",
    "            self.stop_button.config(state=tk.DISABLED)\n",
    "            self.reset_button.config(state=tk.NORMAL)\n",
    "            self.choose_file_button.config(state=tk.NORMAL)  # Re-enable file selection\n",
    "            \n",
    "            # Generate and display summary\n",
    "            self.display_summary()\n",
    "    \n",
    "    def reset_logging(self):\n",
    "        # Reset all variables to initial state\n",
    "        self.emotion_logs = []\n",
    "        self.current_emotions = []\n",
    "        self.logging_start_time = None\n",
    "        self.last_log_time = None\n",
    "        self.elapsed_time = 0\n",
    "        self.lstm_features = []\n",
    "        \n",
    "        # Reset UI\n",
    "        self.time_label.config(text=\"00:00:00\")\n",
    "        self.status_label.config(text=\"Ready\")\n",
    "        self.start_button.config(state=tk.NORMAL)\n",
    "        self.stop_button.config(state=tk.DISABLED)\n",
    "        self.reset_button.config(state=tk.DISABLED)\n",
    "        \n",
    "        # Clear summary\n",
    "        self.summary_text.config(state=tk.NORMAL)\n",
    "        self.summary_text.delete(1.0, tk.END)\n",
    "        self.summary_text.insert(tk.END, \"Ready to start a new session.\\n\")\n",
    "        self.summary_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    def display_summary(self):\n",
    "        self.summary_text.config(state=tk.NORMAL)\n",
    "        self.summary_text.delete(1.0, tk.END)\n",
    "        \n",
    "        if not self.emotion_logs:\n",
    "            self.summary_text.insert(tk.END, \"No emotions logged.\")\n",
    "            self.summary_text.config(state=tk.DISABLED)\n",
    "            return\n",
    "        \n",
    "        # Format final time\n",
    "        hours, remainder = divmod(int(self.elapsed_time), 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        time_str = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "        \n",
    "        self.summary_text.insert(tk.END, f\"== Emotion Logging Summary ==\\n\")\n",
    "        self.summary_text.insert(tk.END, f\"Total Duration: {time_str}\\n\")\n",
    "        \n",
    "        # Add transcript file info if available\n",
    "        if self.transcript_file_path:\n",
    "            filename = os.path.basename(self.transcript_file_path)\n",
    "            self.summary_text.insert(tk.END, f\"Transcript: {filename}\\n\")\n",
    "        \n",
    "        self.summary_text.insert(tk.END, \"\\n\")\n",
    "        \n",
    "        # Per 10-second interval summary\n",
    "        self.summary_text.insert(tk.END, \"=== 10-second Intervals ===\\n\")\n",
    "        for i, log in enumerate(self.emotion_logs):\n",
    "            self.summary_text.insert(tk.END, f\"[{log['timestamp']}] \")\n",
    "            self.summary_text.insert(tk.END, f\"Dominant: {log['dominant_emotion']}\\n\")\n",
    "            \n",
    "            if log['has_negative']:\n",
    "                self.summary_text.insert(tk.END, \"⚠️ Negative emotions detected!\\n\")\n",
    "            \n",
    "            # Show emotion distribution\n",
    "            for emotion, count in log['counts'].items():\n",
    "                percentage = count / sum(log['counts'].values()) * 100\n",
    "                self.summary_text.insert(tk.END, f\"  - {emotion}: {percentage:.1f}%\\n\")\n",
    "            \n",
    "            self.summary_text.insert(tk.END, \"\\n\")\n",
    "        \n",
    "        # Overall summary\n",
    "        self.summary_text.insert(tk.END, \"=== Overall Summary ===\\n\")\n",
    "        \n",
    "        # Count dominant emotions across all intervals\n",
    "        dominant_counts = Counter([log['dominant_emotion'] for log in self.emotion_logs])\n",
    "        most_common = dominant_counts.most_common()\n",
    "        \n",
    "        self.summary_text.insert(tk.END, \"Most frequent emotions:\\n\")\n",
    "        for emotion, count in most_common:\n",
    "            percentage = count / len(self.emotion_logs) * 100\n",
    "            self.summary_text.insert(tk.END, f\"  - {emotion}: {percentage:.1f}%\\n\")\n",
    "        \n",
    "        # Check for negative emotions\n",
    "        negative_intervals = sum(1 for log in self.emotion_logs if log['has_negative'])\n",
    "        if negative_intervals > 0:\n",
    "            neg_percentage = negative_intervals / len(self.emotion_logs) * 100\n",
    "            self.summary_text.insert(tk.END, f\"\\n⚠️ Negative emotions detected in {negative_intervals} intervals ({neg_percentage:.1f}%)\\n\")\n",
    "        else:\n",
    "            self.summary_text.insert(tk.END, \"\\n✅ No negative emotions detected\\n\")\n",
    "        \n",
    "        # Provide presentation advice based on emotions\n",
    "        self.summary_text.insert(tk.END, \"\\n=== Presentation Advice ===\\n\")\n",
    "        \n",
    "        # Tailor advice based on detected emotions\n",
    "        if negative_intervals > len(self.emotion_logs) * 0.3:  # If more than 30% negative\n",
    "            self.summary_text.insert(tk.END, \"Your rehearsal showed significant stress or negative emotions. \")\n",
    "            self.summary_text.insert(tk.END, \"Consider practicing more to build confidence. \")\n",
    "            self.summary_text.insert(tk.END, \"Focus on deep breathing before starting.\\n\")\n",
    "        elif 'Happiness' in dominant_counts and dominant_counts['Happiness'] > len(self.emotion_logs) * 0.4:\n",
    "            self.summary_text.insert(tk.END, \"Great job! You appear confident and positive during your presentation. \")\n",
    "            self.summary_text.insert(tk.END, \"Keep this energy for your actual presentation.\\n\")\n",
    "        elif 'Neutral' in dominant_counts and dominant_counts['Neutral'] > len(self.emotion_logs) * 0.7:\n",
    "            self.summary_text.insert(tk.END, \"Your presentation appears quite neutral. \")\n",
    "            self.summary_text.insert(tk.END, \"Try adding more vocal variety and enthusiasm to engage your audience.\\n\")\n",
    "        else:\n",
    "            self.summary_text.insert(tk.END, \"Your presentation showed mixed emotions. \")\n",
    "            self.summary_text.insert(tk.END, \"Focus on maintaining consistent positive energy throughout.\\n\")\n",
    "        \n",
    "        self.summary_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    def on_closing(self):\n",
    "        self.stop_event.set()\n",
    "        if self.cap and self.cap.isOpened():\n",
    "            self.cap.release()\n",
    "        self.window.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444b94e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 12:14:48.040 Python[92057:4725363] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752347706.083158 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1752347706.085445 4726067 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347706.087915 4726066 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347706.232072 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347706.233314 4726077 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347706.235158 4726077 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347706.363663 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347706.365083 4726082 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347706.367365 4726085 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347706.497500 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347706.498408 4726091 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347706.500479 4726090 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347706.629865 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347706.631148 4726115 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347706.633073 4726114 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347706.763976 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347706.764983 4726122 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347706.767499 4726125 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347706.889148 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347706.890556 4726132 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347706.892367 4726132 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347707.026914 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347707.028162 4726140 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347707.029932 4726140 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347707.160821 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347707.161837 4726148 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347707.164105 4726150 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347707.296851 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347707.297764 4726157 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347707.299467 4726157 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347707.428728 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347707.429681 4726164 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347707.431525 4726165 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347707.562405 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347707.563265 4726174 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347707.564999 4726174 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347707.694327 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347707.695246 4726181 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347707.696946 4726182 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347707.828490 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347707.829428 4726187 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347707.831161 4726190 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347707.961240 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347707.962479 4726198 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347707.964304 4726198 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347708.097046 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347708.097977 4726204 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347708.099820 4726206 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347708.227535 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347708.228866 4726213 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347708.230690 4726212 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347708.372694 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347708.373775 4726219 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347708.375686 4726219 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347708.498296 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347708.499170 4726229 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347708.500716 4726231 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347708.631511 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347708.632442 4726239 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347708.634685 4726243 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347708.763483 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347708.764374 4726264 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347708.766179 4726265 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347708.897416 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347708.898295 4726271 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347708.899992 4726274 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347709.029466 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347709.030359 4726281 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347709.031949 4726284 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347709.164554 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347709.165377 4726288 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347709.167787 4726287 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347709.299013 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347709.300303 4726294 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347709.302414 4726300 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347709.430463 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347709.431410 4726302 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347709.433704 4726304 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752347709.560381 4725363 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1752347709.561966 4726311 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752347709.563712 4726315 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Run the application\n",
    "root = tk.Tk()\n",
    "app = EmotionLoggingApp(root, \"Presentation Rehearsal Emotion Tracker\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
